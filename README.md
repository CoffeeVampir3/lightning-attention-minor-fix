# Lightning Attention

<p align="center">
ðŸ’» <a href="https://github.com/OpenNLPLab/lightning-attention" target="_blank">GitHub </a> â€¢
ðŸ’¬ <a href="https://discord.gg/JEU3nTcWKC" target="_blank">Discord</a> â€¢
ðŸ’¬ <a href="./images/contact_me_qr.png" target="_blank">WeChat</a>
</p>

## Introduction
This repository provides the official implementation of Lightning Attention 1/2 Algorithm.

- [Lightning Attention-1](https://arxiv.org/abs/2307.14995) 
- [Lightning Attention-2](https://arxiv.org/abs/2401.04658)





## Citation
If you wish to cite our work, please use the following reference:
```
@article{qin2023scaling,
  title={Scaling transnormer to 175 billion parameters},
  author={Qin, Zhen and Li, Dong and Sun, Weigao and Sun, Weixuan and Shen, Xuyang and Han, Xiaodong and Wei, Yunshen and Lv, Baohong and Yuan, Fei and Luo, Xiao and others},
  journal={arXiv preprint arXiv:2307.14995},
  year={2023}
}

@misc{qin2024lightning,
      title={Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models}, 
      author={Zhen Qin and Weigao Sun and Dong Li and Xuyang Shen and Weixuan Sun and Yiran Zhong},
      year={2024},
      eprint={2401.04658},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

```